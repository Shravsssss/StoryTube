# Panda 3D

# NLP:

The system uses NLP techniques to extract visualizable information such as:

i) Physical objects, their properties, and their constraints.

ii) Actions performed by the story actors.

iii) Dialogues between the actors.

iv) Emotions of the actors.

v) Weather conditions.

vi) Background scene.

It is assumed the scene takes place in **same location**.

# **Step 1: Extracting Objects, Properties, and Constraints**

- Extract objects by first finding all nouns using POS tagger. Among these, physical objects are identified using WordNet database
- Identify properties such as count, color, hair, shape, etc y identifying noun phrase of each noun, then extracting existing properties from the noun phrase
- Discover the constraints between the objects by identifying the prepositions (e.g., on, below, and behind) and their relevant objects

# **Step 2: Event Extraction and Ordering**

- Convert complex sentences to simple ones
- Classify sentences into action and non-action sentences using VerbNet.
- Action sentences further processed by action extraction component.
- Non-action sentences are then further processed by the Weather Extraction and Emotion Extraction components

## **2.1: Action Extraction:**

System extracts all actions that can be visualised by:

1. Using POS tagger to assign tags
2. Non action verbs are identified by VerbNet to be ignored in visualization process
3. Sentences with action verbs are further processed to identify all the relevant information about their actions
4. Compound subjects, objects, and verbs are supported as well. A compound subject consists of multiple subjects performing the same action (e.g., Mary, John, and Alice were playing with the ball.). A compound object consists of multiple objects being acted upon (e.g., Mary was carrying an apple and an orange.). ***System handles this by following conjunctions to find the actual POS using the sentence’s dependency parse tree.***
5. *A special kind of action is when characters communicate with each other in the story.* F*or this, we use some techniques to detect the conversations, and the speakers are detected as explained earlier*. ***Quotes*** (when they exist) are used as a strong feature to detect the speech segments. ***Verbs implying speech*** (e.g., say and shout) and the ***first succeeding non-speech verb*** (if exists) are detected. Afterwards, ***the sentence is split*** at the subject associated with the non-speech verb. For example, in the sentence “John said that the weather is fine.”, the sentence is split at “the weather”, and “the weather is fine” is used as the extracted speech segment.
6. Verbs can have diff meanings based on context. We develop some linguistic rules for common verbs to handle their different meanings. For example, run could only imply motion in case it occurs as an intransitive verb.

## **2.2: Weather extraction**

Done using wordnet. System queries knowledge base and detects whether the weather applies to entire story or just middle

## **2.3: Emotion extraction**

Some emotions are not related to a certain action (e.g., He got angry.), hence they are not handled during action extraction. These emotions are extracted as follows: i) Use a ***POS tagger*** to extract nouns, adverbs, and adjectives.

ii) Extract the ***synonyms*** of these words using WordNet.

iii) Determine if each word or any of its ***synonyms is emotion-related*** or ***not*** using a knowledge base.

iv) ***Categorize each word*** into six categories using an emotion lexicon: fear, joy, anger, surprise, disgust, and sadness.

## **2.4: Event ordering**

After extracting events, we need to determine their chronological order.Story events are sequential unless special keywords appear. We use keywords such as “then” and “after” to determine the correct sequential order of events. Additionally, keywords such as “while”, “when” and “at the same time” are used to determine action parallelism.

# **Step 3: Scene identification**

System identifies general scene among house, street, farm, beach and garden.

### **Case 1: Scene explicitly mentioned**

System uses rule based techniques to identify the scene and exclude scenes mentioned in speech sentences or negated sentences

### **Case 2: Not explicitly mentioned**

System uses Naive Bayes classifier to find probability of each scene given the probability of having these objects and actions in this particular scene.

![Untitled](Panda%203D%20f6fdd822b7b043c997a51e71522dd95a/Untitled.png)

# Animation:

The system produces the animated 3D video by constructing the scene, positioning the objects, scheduling the events, animating the scene, and overlaying the audio of the dialogues. 
A representation that is flexible and extendible is passed from the NLP pipeline
to the graphics pipeline, and it includes:
• Identified scene, e.g., garden or room.
• Weather information, e.g., sunny or rainy.
• Time of the day, e.g., day or night.
• Static objects (e.g., cup), their counts and colors.
• Actors (i.e., objects that can move, e.g., boy or
dog) and their hair and eye colors.
• Positional constraints (e.g., beside or above) between the objects and the actors in the scene.
• Actions where each action can be:
– Single: An action performed by a single actor.
– Aggregate: An action performed by a group of
actors together, e.g., dancing. The verb associated with each action (single or aggregate) can
be: a verb with no objects (e.g., run), a verb
with direct objects, or a verb with direct and indirect objects.
– Event: Represents changes in weather conditions (e.g., It suddenly rained.) or actors’ emotions (e.g., He became sad.).
– Parallel: A set of actions (single, aggregate, or
event) that are happening at the same time. For
example, the boy was running while the girl
was eating.

# 3.1 Static scene setup

A custom database of set of background images at diff times of the day like morning and evening exists. It also contains different terrains. Objects and actors are stored with metadata such as gender and type. The main scene elements are retrieved to setup the static scene.

# 3.2 Object positioning

Handles binary and non-binary constraints by converting non-b to set of binary constraints. The paper has an algorithm for positioning. 
X axis is left to right, Y axis is into the screen and Z is bottom to top. So its not the standard XY. 
In essence, it first creates an undirected graph where vertices are the objects and edges are the XY constraints, which determine which object is beside another, and also which object is in front of the other. 
Next, it finds strongly connected components and builds a new directed graph where the SCCs are the nodes and Z constraints are the vertices. 
Finally a topological sort is performed so that parent SCCs are positioned before children. Any necessary scaling is performed as the last step.

# 3.3 Scene animation

Action can be single, aggregate, event of any of these in parallel.
Each action has a precondition, an execution mechanism, and termination condition.

## Single action

- Precondition: The precondition is required to start executing
the main action. For example: i) To give an object to an actor, a precondition would be to pick
the object. ii) To pick an object, a precondition would be to walk towards the location in which
the object exists in the scene. iii) To walk towards an object, the actor must be in a standing posture (not sitting on a chair for example).
- The execution of the action is playing a representative animation until the termination
condition is met.
    - For actions such as walk, run, and wander, the execution requires the **invocation of steering behaviors** that ensure the smooth navigation of actors in the scene.
    - For actions that imply speech such as say and talk, a **third party text-to-speech service** is used to allow actors to speak in voices that reflect their gender along with **speech text in speech bubbles** above the actor.
    - For actions such as bark and meow that are usually associated with animal actors, **animal noises** are played
- Termination condition marks the end of action. For example, for the pick action, the termination condition is that the actor has the object in his grip. For some actions such as wander, it may be difficult to have a clear termination condition. For this, termination condition is defined as as a time interval after which the action should stop executing.

## Aggregate action

For aggregate actions that are performed by multiple actors, the system replicates the single action for all actors.

## Events

Events can be:
1) Weather change
2) Emotion

1) For weather change, rainy and snowy are simulated using particle effects, and cloudy is simulated by randomly moving cloud models.

2) For emotion events, emojis are loaded above the actor in order to indicate the desired emotion.

## Navigation

The navigation of the actors around the scene is achieved by implementing Reynolds’ steering behaviors (Reynolds, 1999) which determine the navigation path while not making assumptions about the scene or the actors. Objects are treated as a single point mass. Each object is characterized by a velocity that is modified by applying a force.

3 behaviours are implemented:

- Seek - moving towards a point
- Wander - random steering implemented by generating random forces to simulate the wandering of actors
- Obstacle avoidance - generating forces that allow actor to steer away from nearby objects

# 3.4 Objects database

Database of models is collected from various online sources and MongoDB is used to store metadata about the objects.

Metadata - type, dimensions, names of animation it supports.

Mixamo is used to support multiple animations

Mixamo’s free models are also used to expand the database.

Panda3D is used for rendering.