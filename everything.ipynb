{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coref resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clsuters:-\n",
      "[[0, 1], [9, 9], [12, 12], [18, 18]]\n",
      "[[21, 21], [29, 29]]\n",
      "\n",
      "\n",
      "\n",
      "Coref resolved:  A cat is walking on a sunny day. A cat jumped. A cat ran over a stone. A cat died. Dog is walking in the opposite direction. Dog ran.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load saved model\n",
    "with open('allen_coref' , 'rb') as f:\n",
    "    predictor = pickle.load(f)\n",
    "\n",
    "text = \"A cat is walking on a sunny day. It jumped. It ran over a stone. It died. Dog is walking in the opposite direction. It ran.\"\n",
    "\n",
    "prediction = predictor.predict(document=text)  # get prediction\n",
    "print(\"Clsuters:-\")\n",
    "for cluster in prediction['clusters']:\n",
    "    print(cluster)  # list of clusters (the indices of spaCy tokens)\n",
    "# Result: [[[0, 3], [26, 26]], [[34, 34], [50, 50]]]\n",
    "print('\\n\\n') #Newline\n",
    "\n",
    "coref_text = predictor.coref_resolved(text)\n",
    "\n",
    "print('Coref resolved: ', coref_text)  # resolved text\n",
    "# Result: Joseph Robinette Biden Jr. is an American politician who is the 46th and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAO parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shour\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.4.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is walking on a sunny day. A cat jumped. A cat ran over a stone. A cat died. Dog is walking in the opposite direction. Dog ran.\n",
      "[('A cat', 'walking'), ('A cat', 'jumped'), ('A cat', 'ran'), ('A cat', 'died'), ('Dog', 'walking'), ('Dog', 'ran')]\n"
     ]
    }
   ],
   "source": [
    "from subject_verb_object_extract import findSVOs, nlp\n",
    "# coref_text = \"A cat is walk rapidly on a sunny day. A cat fall on a stone. A cat dead\"\n",
    "tokens = nlp(coref_text)\n",
    "SVOs = findSVOs(tokens)\n",
    "print(coref_text)\n",
    "print(SVOs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': ['walking', 'jumped', 'ran', 'died'], 'dog': ['walking', 'ran']}\n",
      "['cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "stop_words = set(stopwords.words('english'))\n",
    "svos = defaultdict(list)\n",
    "\n",
    "for svo in SVOs: \n",
    "    subject = svo[0] if len(svo) == 2 else svo[2]\n",
    "    word_tokens = word_tokenize(subject)\n",
    "    filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
    "    svos[\"\".join(filtered_sentence)].append(svo[1])\n",
    "    \n",
    "svos = dict(svos)\n",
    "print(svos)\n",
    "\n",
    "charcaters = list(i.lower() for i in svos.keys())\n",
    "print(charcaters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "actions_movement = {'die' : 0, 'fall' : 0, 'hurt' : 0, 'idle' : 0, 'jump' : 1, 'run' : 1, 'slide' : 1, 'walk' : 1}\n",
    "#word2vec similarity between the incoming action vs the ones we have in list of actions and then set a threshold and execute the action based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\shour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "jump\n",
      "run\n",
      "die\n",
      "walk\n",
      "run\n",
      "{'cat': ['walking', 'jumped', 'ran', 'died'], 'dog': ['walking', 'ran']}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "for actions in list(itertools.chain(*svos.values())):\n",
    "    action = WordNetLemmatizer().lemmatize(actions,'v')\n",
    "    print(action)\n",
    "\n",
    "print(svos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walk']\n",
      "['walk', 'jump']\n",
      "['walk', 'jump', 'run']\n",
      "['walk', 'jump', 'run', 'die']\n",
      "{'walk': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'jump': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'run': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'die': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>]}\n",
      "['walk']\n",
      "['walk', 'run']\n",
      "{'walk': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'run': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>]}\n",
      "0 0 4 10\n",
      "0\n",
      "0 0 2 10\n",
      "0\n",
      "1 0 4 10\n",
      "0\n",
      "1 0 2 10\n",
      "0\n",
      "2 0 4 10\n",
      "0\n",
      "2 0 2 10\n",
      "0\n",
      "3 0 4 10\n",
      "0\n",
      "3 0 2 10\n",
      "0\n",
      "4 0 4 10\n",
      "0\n",
      "4 0 2 10\n",
      "0\n",
      "5 0 4 10\n",
      "0\n",
      "5 0 2 10\n",
      "0\n",
      "6 0 4 10\n",
      "0\n",
      "6 0 2 10\n",
      "0\n",
      "7 0 4 10\n",
      "0\n",
      "7 0 2 10\n",
      "0\n",
      "8 0 4 10\n",
      "0\n",
      "8 0 2 10\n",
      "0\n",
      "9 0 4 10\n",
      "0\n",
      "9 0 2 10\n",
      "0\n",
      "10 0 4 10\n",
      "0\n",
      "10 0 2 10\n",
      "0\n",
      "1 1 4 8\n",
      "0\n",
      "1 1 2 8\n",
      "0\n",
      "2 1 4 8\n",
      "0\n",
      "2 1 2 8\n",
      "0\n",
      "3 1 4 8\n",
      "0\n",
      "3 1 2 8\n",
      "0\n",
      "4 1 4 8\n",
      "0\n",
      "4 1 2 8\n",
      "0\n",
      "5 1 4 8\n",
      "0\n",
      "5 1 2 8\n",
      "0\n",
      "6 1 4 8\n",
      "0\n",
      "6 1 2 8\n",
      "0\n",
      "7 1 4 8\n",
      "0\n",
      "7 1 2 8\n",
      "0\n",
      "8 1 4 8\n",
      "0\n",
      "8 1 2 8\n",
      "1\n",
      "728\n",
      "True\n",
      "1 2 4 8\n",
      "0\n",
      "728\n",
      "True\n",
      "2 2 4 8\n",
      "0\n",
      "728\n",
      "True\n",
      "3 2 4 8\n",
      "0\n",
      "728\n",
      "True\n",
      "4 2 4 8\n",
      "0\n",
      "728\n",
      "True\n",
      "5 2 4 8\n",
      "0\n",
      "728\n",
      "True\n",
      "6 2 4 8\n",
      "0\n",
      "728\n",
      "True\n",
      "7 2 4 8\n",
      "0\n",
      "728\n",
      "True\n",
      "8 2 4 8\n",
      "0\n",
      "728\n",
      "True\n",
      "1 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "2 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "3 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "4 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "5 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "6 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "7 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "8 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "9 3 4 10\n",
      "0\n",
      "728\n",
      "True\n",
      "10 3 4 10\n",
      "1\n",
      "728\n",
      "True\n",
      "304\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from numpy import character\n",
    "import pygame\n",
    "import glob\n",
    "from time import sleep\n",
    " \n",
    "SIZE = WIDTH, HEIGHT = 1280, 720 #the width and height of our screen\n",
    "FPS = 4 #Frames per second\n",
    "\n",
    "screen = pygame.display.set_mode(SIZE)\n",
    "bg = pygame.transform.scale(pygame.image.load('pygame\\sunny_day.png') , SIZE)\n",
    "\n",
    "dir_list = {'r' : 200, 'l' : 400} #moving towards which direction\n",
    " \n",
    "class MySprite(pygame.sprite.Sprite):\n",
    "    def __init__(self, char, x, y, dir):\n",
    "        super(MySprite, self).__init__()\n",
    "        \n",
    "        self.images = {}\n",
    "        self.actions = []\n",
    "        # self.images = [pygame.image.load(img) for img in glob.glob(\"pygame\\\\cat\\\\*.png\")]\n",
    "        for action in svos[char]:\n",
    "            action = WordNetLemmatizer().lemmatize(action,'v')            \n",
    "            self.actions.append(action)\n",
    "            print(self.actions)\n",
    "            \n",
    "            if action not in self.images:\n",
    "                self.images[action] = [pygame.transform.scale(pygame.image.load(img) , (200,200)) for img in glob.glob(\"pygame\\\\\" + char + \"\\\\\" + action + \"*.png\")]\n",
    "            \n",
    "        print(self.images)\n",
    "        self.char = char\n",
    "        self.index = 0\n",
    "        self.action_count = 0\n",
    "        self.dir = dir\n",
    "        self.x = x\n",
    "        self.y = y   \n",
    "\n",
    "    def movement_update(self):\n",
    "        # if self.x == 880:\n",
    "        #     self.dir = 'l'\n",
    "        # elif self.x == 0:\n",
    "        #     self.dir = 'r'\n",
    "\n",
    "        if self.dir == 'r':                        \n",
    "            self.x += FPS\n",
    "        else:\n",
    "            self.image = pygame.transform.flip(self.image, True, False)\n",
    "            self.x -= FPS\n",
    "        screen.blit(self.image, (self.x, self.y))\n",
    " \n",
    "    def update(self):\n",
    "        print(self.index, self.action_count, len(self.actions), len(self.images[self.actions[self.action_count]]))\n",
    "        if self.index >= len(self.images[self.actions[self.action_count]]):\n",
    "            if self.action_count >= len(self.actions) - 1:\n",
    "                return 1\n",
    "            self.index = 0\n",
    "            self.action_count += 1\n",
    "        self.image = self.images[self.actions[self.action_count]][self.index]\n",
    "        self.index += 1\n",
    "\n",
    "        if actions_movement[self.actions[self.action_count]]:\n",
    "            self.movement_update()\n",
    "        else:\n",
    "            screen.blit(self.image, (self.x, self.y))\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def update_idle(self):\n",
    "        self.image = pygame.transform.scale(pygame.image.load(\"pygame\\\\\" + self.char + \"\\\\\" + \"idle1.png\") , (200,200))\n",
    "        flip_var = True if self.dir == \"l\" else False\n",
    "        print(flip_var)\n",
    "        self.image = pygame.transform.flip(self.image, flip_var, False)\n",
    "        screen.blit(self.image, (self.x, self.y))\n",
    "        \n",
    " \n",
    "def main():\n",
    "    pygame.init()\n",
    "    pygame.display.set_caption(\"Trace\")\n",
    "    char_objects = []\n",
    "    for i in range(len(charcaters)):\n",
    "        dir = list(dir_list.keys())[i % 2]\n",
    "        char_objects.append(MySprite(charcaters[i], dir_list[dir] * ((i % 2) + 1), 350, dir))\n",
    "    # my_group = pygame.sprite.Group(my_sprite)\n",
    "    idle_char = []\n",
    "    clock = pygame.time.Clock()\n",
    "    \n",
    "    loop = 1\n",
    "    count = 0 # number of characters whos actions have been completed\n",
    "\n",
    "\n",
    "    return_val = 0\n",
    "    while count < len(charcaters): \n",
    "\n",
    "        pygame.event.get() \n",
    "        \n",
    "        screen.fill((0,0,0))        \n",
    "        screen.blit(bg, (0, 0))\n",
    "\n",
    "        for char in char_objects:\n",
    "            return_val = char.update()   \n",
    "            print(return_val) \n",
    "            if return_val == 1:\n",
    "                char_objects.remove(char)\n",
    "                idle_char.append(char)\n",
    "                count += return_val\n",
    "        \n",
    "        for char in idle_char:\n",
    "            print(char.x)\n",
    "            char.update_idle()\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(FPS)\n",
    "\n",
    "    pygame.quit()\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import spacy.cli\n",
    "# Download following once\n",
    "#spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CS', 24, 26, 'ORG'), ('MIT', 48, 51, 'ORG')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Victoria was a major in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Victoria studies at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MIT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Victoria was a major in CS. Victoria studies at MIT.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "# now we use displaycy function on doc2\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is walking on a sunny day. A cat jumped. A cat was talking with a human and died. Dog is walking in the opposite direction. Dog ran.\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token is POS: AUX, dep: aux\n",
      "Token walking POS: VERB, dep: ROOT\n",
      "Token on POS: ADP, dep: prep\n",
      "Token a POS: DET, dep: det\n",
      "Token sunny POS: ADJ, dep: amod\n",
      "Token day POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token jumped POS: VERB, dep: ROOT\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token was POS: AUX, dep: aux\n",
      "Token talking POS: VERB, dep: ROOT\n",
      "Token with POS: ADP, dep: prep\n",
      "Token a POS: DET, dep: det\n",
      "Token human POS: NOUN, dep: pobj\n",
      "Token and POS: CCONJ, dep: cc\n",
      "Token died POS: VERB, dep: conj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token Dog POS: PROPN, dep: nsubj\n",
      "Token is POS: AUX, dep: aux\n",
      "Token walking POS: VERB, dep: ROOT\n",
      "Token in POS: ADP, dep: prep\n",
      "Token the POS: DET, dep: det\n",
      "Token opposite POS: ADJ, dep: amod\n",
      "Token direction POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token Dog POS: PROPN, dep: nsubj\n",
      "Token ran POS: VERB, dep: ROOT\n",
      "Token . POS: PUNCT, dep: punct\n",
      "svo:, subject: a cat a cat a cat dog dog, verb: walking jumped talking died walking ran, attribute: , question: False, wh_word: \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# object and subject constants\n",
    "OBJECT_DEPS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"agent\", \"expl\"}\n",
    "# tags that define wether the word is wh-\n",
    "WH_WORDS = {\"WP\", \"WP$\", \"WRB\"}\n",
    "\n",
    "# extract the subject, object and verb from the input\n",
    "def extract_svo(doc):\n",
    "    sub = []\n",
    "    at = []\n",
    "    ve = []\n",
    "    for token in doc:\n",
    "        # is this a verb?\n",
    "        if token.pos_ == \"VERB\":\n",
    "            ve.append(token.text)\n",
    "        # is this the object?\n",
    "        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:\n",
    "            at.append(token.text)\n",
    "        # is this the subject?\n",
    "        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS:\n",
    "            sub.append(token.text)\n",
    "    return \" \".join(sub).strip().lower(), \" \".join(ve).strip().lower(), \" \".join(at).strip().lower()\n",
    "\n",
    "# wether the doc is a question, as well as the wh-word if any\n",
    "def is_question(doc):\n",
    "    # is the first token a verb?\n",
    "    if len(doc) > 0 and doc[0].pos_ == \"VERB\":\n",
    "        return True, \"\"\n",
    "    # go over all words\n",
    "    for token in doc:\n",
    "        # is it a wh- word?\n",
    "        if token.tag_ in WH_WORDS:\n",
    "            return True, token.text.lower()\n",
    "    return False, \"\"\n",
    "\n",
    "# gather the user input and gather the info\n",
    "doc = nlp(\"A cat is walking on a sunny day. A cat jumped. A cat was talking with a human and died. Dog is walking in the opposite direction. Dog ran.\")\n",
    "print(doc)\n",
    "# print out the pos and deps\n",
    "for token in doc:\n",
    "    print(\"Token {} POS: {}, dep: {}\".format(token.text, token.pos_, token.dep_))\n",
    "\n",
    "# get the input information\n",
    "subject, verb, attribute = extract_svo(doc)\n",
    "question, wh_word = is_question(doc)\n",
    "print(\"svo:, subject: {}, verb: {}, attribute: {}, question: {}, wh_word: {}\".format(subject, verb, attribute, question, wh_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
