{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coref resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: allennlp_models in d:\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: allennlp<2.11,>=2.10.0 in d:\\lib\\site-packages (from allennlp_models) (2.10.0)\n",
      "Requirement already satisfied: nltk>=3.6.5 in d:\\lib\\site-packages (from allennlp_models) (3.7)\n",
      "Requirement already satisfied: ftfy in d:\\lib\\site-packages (from allennlp_models) (6.1.1)\n",
      "Requirement already satisfied: torch<1.12.0,>=1.7.0 in d:\\lib\\site-packages (from allennlp_models) (1.11.0)\n",
      "Requirement already satisfied: py-rouge==1.1 in d:\\lib\\site-packages (from allennlp_models) (1.1)\n",
      "Requirement already satisfied: datasets in d:\\lib\\site-packages (from allennlp_models) (2.4.0)\n",
      "Requirement already satisfied: word2number>=1.1 in d:\\lib\\site-packages (from allennlp_models) (1.1)\n",
      "Requirement already satisfied: conllu==4.4.2 in d:\\lib\\site-packages (from allennlp_models) (4.4.2)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (2.5.1)\n",
      "Requirement already satisfied: protobuf==3.20.0 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (3.20.0)\n",
      "Requirement already satisfied: filelock<3.8,>=3.3 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (3.7.1)\n",
      "Requirement already satisfied: pytest>=6.2.5 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (7.1.2)\n",
      "Requirement already satisfied: traitlets>5.1.1 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (5.3.0)\n",
      "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.12.21)\n",
      "Requirement already satisfied: lmdb>=1.2.1 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.3.0)\n",
      "Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.1.2)\n",
      "Requirement already satisfied: transformers<4.21,>=4.1 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (4.20.1)\n",
      "Requirement already satisfied: spacy<3.4,>=2.1.0 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (3.3.1)\n",
      "Requirement already satisfied: fairscale==0.4.6 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.4.6)\n",
      "Requirement already satisfied: dill>=0.3.4 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.3.5.1)\n",
      "Requirement already satisfied: more-itertools>=8.12.0 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (8.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.16 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.21.4 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.23.2)\n",
      "Requirement already satisfied: torchvision<0.13.0,>=0.8.1 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.12.0)\n",
      "Requirement already satisfied: rich==12.1 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (12.1.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.1.97)\n",
      "Requirement already satisfied: typer>=0.4.1 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.4.2)\n",
      "Requirement already satisfied: termcolor==1.1.0 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.1.0)\n",
      "Requirement already satisfied: sacremoses in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.0.53)\n",
      "Requirement already satisfied: h5py>=3.6.0 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (3.7.0)\n",
      "Requirement already satisfied: requests>=2.28 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (2.28.1)\n",
      "Requirement already satisfied: scipy>=1.7.3 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.9.1)\n",
      "Requirement already satisfied: base58>=2.1.1 in d:\\lib\\site-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (2.1.1)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in d:\\lib\\site-packages (from rich==12.1->allennlp<2.11,>=2.10.0->allennlp_models) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in d:\\lib\\site-packages (from rich==12.1->allennlp<2.11,>=2.10.0->allennlp_models) (2.12.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\lib\\site-packages (from nltk>=3.6.5->allennlp_models) (2022.3.15)\n",
      "Requirement already satisfied: click in d:\\lib\\site-packages (from nltk>=3.6.5->allennlp_models) (8.0.3)\n",
      "Requirement already satisfied: joblib in d:\\lib\\site-packages (from nltk>=3.6.5->allennlp_models) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\lib\\site-packages (from torch<1.12.0,>=1.7.0->allennlp_models) (4.1.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\lib\\site-packages (from datasets->allennlp_models) (2022.7.1)\n",
      "Requirement already satisfied: pandas in d:\\lib\\site-packages (from datasets->allennlp_models) (1.2.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in d:\\lib\\site-packages (from datasets->allennlp_models) (9.0.0)\n",
      "Requirement already satisfied: packaging in d:\\lib\\site-packages (from datasets->allennlp_models) (21.3)\n",
      "Requirement already satisfied: xxhash in d:\\lib\\site-packages (from datasets->allennlp_models) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in d:\\lib\\site-packages (from datasets->allennlp_models) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in d:\\lib\\site-packages (from datasets->allennlp_models) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in d:\\lib\\site-packages (from datasets->allennlp_models) (0.18.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in d:\\lib\\site-packages (from ftfy->allennlp_models) (0.2.5)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in d:\\lib\\site-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.24.61)\n",
      "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in d:\\lib\\site-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (2.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\lib\\site-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.0->allennlp_models) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\lib\\site-packages (from packaging->datasets->allennlp_models) (2.4.7)\n",
      "Requirement already satisfied: py>=1.8.2 in d:\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (1.11.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in d:\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in d:\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (1.1.1)\n",
      "Requirement already satisfied: colorama in d:\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (0.4.4)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in d:\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (1.0.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in d:\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (1.4.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in d:\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\lib\\site-packages (from requests>=2.28->allennlp<2.11,>=2.10.0->allennlp_models) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lib\\site-packages (from requests>=2.28->allennlp<2.11,>=2.10.0->allennlp_models) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\lib\\site-packages (from requests>=2.28->allennlp<2.11,>=2.10.0->allennlp_models) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lib\\site-packages (from requests>=2.28->allennlp<2.11,>=2.10.0->allennlp_models) (2.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\lib\\site-packages (from scikit-learn>=1.0.1->allennlp<2.11,>=2.10.0->allennlp_models) (3.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.0.8)\n",
      "Requirement already satisfied: jinja2 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (3.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (0.7.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.4.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (8.0.17)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (3.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (0.6.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (3.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.8.2)\n",
      "Requirement already satisfied: setuptools in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (56.0.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.0.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\lib\\site-packages (from torchvision<0.13.0,>=0.8.1->allennlp<2.11,>=2.10.0->allennlp_models) (8.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in d:\\lib\\site-packages (from transformers<4.21,>=4.1->allennlp<2.11,>=2.10.0->allennlp_models) (0.12.1)\n",
      "Requirement already satisfied: pathtools in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (0.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.9.5)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (3.1.27)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (5.9.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.3.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.0.9)\n",
      "Requirement already satisfied: six>=1.13.0 in d:\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.16.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\lib\\site-packages (from aiohttp->datasets->allennlp_models) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\lib\\site-packages (from aiohttp->datasets->allennlp_models) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\lib\\site-packages (from aiohttp->datasets->allennlp_models) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\lib\\site-packages (from aiohttp->datasets->allennlp_models) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\lib\\site-packages (from aiohttp->datasets->allennlp_models) (1.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\lib\\site-packages (from pandas->datasets->allennlp_models) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\lib\\site-packages (from pandas->datasets->allennlp_models) (2.8.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.61 in d:\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.27.61)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in d:\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (0.6.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\lib\\site-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (4.0.9)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in d:\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (2.8.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in d:\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (2.9.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in d:\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in d:\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (2.3.3)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in d:\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\lib\\site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.0.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (5.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in d:\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.56.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in d:\\lib\\site-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install allennlp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clsuters:-\n",
      "[[3, 4], [6, 6], [18, 18], [26, 26], [47, 48], [67, 67], [89, 89], [101, 101], [103, 103], [110, 110], [114, 114]]\n",
      "[[22, 23], [31, 31]]\n",
      "[[34, 34], [42, 42], [51, 52], [55, 55], [57, 57], [62, 63]]\n",
      "[[80, 80], [83, 83]]\n",
      "\n",
      "\n",
      "\n",
      "Coref resolved:  Once there lived a crow. a crow wished to be colorful and beautiful like other birds. \n",
      "a crow then went to the parrot and shared a crow's thoughts. \n",
      "But the parrot said \"Peacock is most beautiful bird so talk to Peacock\". \n",
      "Then a crow went to Peacock and told Peacock about Peacock's looks. \n",
      "Then Peacock replied, \"a crow are the luckiest bird that has been never caged in life and we because of we's beauty stay caged, and a crow are always free.\"\n",
      "After listening to this, a crow realized a crow's mistake and thanked God for making a crow like this and a crow flew away happily.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path, PosixPath, WindowsPath\n",
    "\n",
    "path = WindowsPath('./allen_coref')\n",
    "# load saved model\n",
    "with open(path , 'rb') as f:\n",
    "    predictor = pickle.load(f)\n",
    "\n",
    "#text = \"\"\"A cat is walking on a sunny day. It jumped over a stone. It died. Dog is walking in the opposite direction. It ran. The dog said \"Hello world\".\"\"\"\n",
    "text = \"\"\"Once there lived a crow. He wished to be colorful and beautiful like other birds. \n",
    "He then went to the parrot and shared his thoughts. \n",
    "But parrot said \"Peacock is most beautiful bird so talk to him\". \n",
    "Then the crow went to the peacock and told him about his looks. \n",
    "Then the peacock replied, \"You are the luckiest bird that has been never caged in life and we because of our beauty stay caged, and you are always free.\"\n",
    "After listening to this, crow realized his mistake and thanked God for making him like this and he flew away happily.\"\"\"\n",
    "prediction = predictor.predict(document=text)  # get prediction\n",
    "print(\"Clsuters:-\")\n",
    "for cluster in prediction['clusters']:\n",
    "    print(cluster)  # list of clusters (the indices of spaCy tokens)\n",
    "# Result: [[[0, 3], [26, 26]], [[34, 34], [50, 50]]]\n",
    "print('\\n\\n') #Newline\n",
    "\n",
    "coref_text = predictor.coref_resolved(text)\n",
    "\n",
    "print('Coref resolved: ', coref_text)  # resolved text\n",
    "# Result: Joseph Robinette Biden Jr. is an American politician who is the 46th and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAO parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textacy\n",
    "# !pip install stanford_openie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from subject_verb_object_extract import findSVOs, nlp\n",
    "# coref_text = \"A cat is walk rapidly on a sunny day. A cat fall on a stone. A cat dead\"\n",
    "# tokens = nlp(coref_text)\n",
    "# SVOs = findSVOs(tokens)\n",
    "# print(coref_text)\n",
    "# print(SVOs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': ['walking', 'jumped', 'died'], 'dog': ['walking', 'ran', ['said', 'Hello world']]}\n",
      "['cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from collections import defaultdict\n",
    "# from nltk.tokenize import word_tokenize\n",
    "  \n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# svos = defaultdict(list)\n",
    "\n",
    "# for svo in SVOs: \n",
    "#     subject = svo[0]\n",
    "#     word_tokens = word_tokenize(subject)\n",
    "#     filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
    "#     if svo[1] == \"said\":\n",
    "#         verb = []\n",
    "#         verb.append(svo[1])\n",
    "#         verb.append(svo[2])\n",
    "#     else:\n",
    "#         verb = svo[1]\n",
    "#     svos[\"\".join(filtered_sentence)].append(verb)\n",
    "    \n",
    "# svos = dict(svos)\n",
    "# print(svos)\n",
    "\n",
    "# charcaters = list(i.lower() for i in svos.keys())\n",
    "# print(charcaters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClausIE installation\n",
    "python -m pip install git+https://github.com/mmxgn/spacy-clausie.git\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "## ClausIE usage\n",
    "A clause is a part of a sentence that expresses some coherent piece of information; it consists of one subject (S), one\n",
    "verb (V), and optionally of an indirect object (O), a direct\n",
    "object (O), a complement (C), and one or more adverbials\n",
    "(A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<SV, A cat, is walk, None, None, None, [rapidly, on a sunny day]>, <SVA, A cat, fall, None, None, None, [on a stone]>]\n",
      "[['A cat is walk rapidly on a sunny day', 'A cat is walk on a sunny day', 'A cat is walk rapidly'], ['A cat fell on a stone']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import claucy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "claucy.add_to_pipe(nlp)\n",
    "\n",
    "coref_text = \"A cat is walk rapidly on a sunny day. A cat fall on a stone. A cat dead\"\n",
    "doc = nlp(coref_text)\n",
    "\n",
    "SVOs = doc._.clauses\n",
    "print(SVOs)\n",
    "\n",
    "propositions = [SVO.to_propositions(as_text=True) for SVO in SVOs]\n",
    "\n",
    "#print(propositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "actions_movement = {'die' : 0, 'fall' : 0, 'hurt' : 0, 'idle' : 0, 'jump' : 1, 'run' : 1, 'slide' : 1, 'walk' : 1, 'say': 0}\n",
    "#word2vec similarity between the incoming action vs the ones we have in list of actions and then set a threshold and execute the action based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"said\",'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walk']\n",
      "['walk', 'jump']\n",
      "['walk', 'jump', 'die']\n",
      "{'walk': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'jump': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'die': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>]}\n",
      "['walk']\n",
      "['walk', 'run']\n",
      "['walk', 'run', 'say']\n",
      "{'walk': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'run': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'say': [<Surface(200x200x32 SW)>]}\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "die\n",
      "0\n",
      "say\n",
      "dialogie played\n",
      "1\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "1\n",
      "674\n",
      "True\n",
      "326\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from numpy import character\n",
    "import pygame\n",
    "import glob\n",
    "from time import sleep\n",
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "SIZE = WIDTH, HEIGHT = 1280, 720 #the width and height of our screen\n",
    "FPS = 7 #Frames per second\n",
    "\n",
    "screen = pygame.display.set_mode(SIZE)\n",
    "bg = pygame.transform.scale(pygame.image.load('pygame\\sunny_day.png') , SIZE)\n",
    "\n",
    "dir_list = {'r' : 200, 'l' : 400} #moving towards which direction\n",
    "\n",
    "dialogues = []\n",
    "dialogue_count = 0\n",
    " \n",
    "class MySprite(pygame.sprite.Sprite):\n",
    "\n",
    "    def __init__(self, char, x, y, dir):\n",
    "        super(MySprite, self).__init__()\n",
    "        \n",
    "        self.images = {}\n",
    "        self.actions = []\n",
    "        # self.images = [pygame.image.load(img) for img in glob.glob(\"pygame\\\\cat\\\\*.png\")]\n",
    "        for action in svos[char]:\n",
    "            if type(action) == list:\n",
    "                dialogues.append(action[1])\n",
    "            action = WordNetLemmatizer().lemmatize(action[0] if type(action) == list else action,'v')   \n",
    "            self.actions.append(action)\n",
    "            print(self.actions)\n",
    "            \n",
    "            if action not in self.images:\n",
    "                self.images[action] = [pygame.transform.scale(pygame.image.load(img) , (200,200)) for img in glob.glob(\"pygame\\\\\" + char + \"\\\\\" + action + \"*.png\")]\n",
    "            \n",
    "        print(self.images)\n",
    "        self.char = char\n",
    "        self.index = 0\n",
    "        self.action_count = 0\n",
    "        self.dir = dir\n",
    "        self.x = x\n",
    "        self.y = y   \n",
    "\n",
    "    def play_dialogue(self, index):\n",
    "        myobj = gTTS(text=dialogues[index], lang=language, slow=False)\n",
    "        myobj.save(\"dialogue.mp3\")\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(\"dialogue.mp3\")\n",
    "        pygame.mixer.music.set_volume(1)\n",
    "        pygame.mixer.music.play()\n",
    "        \n",
    "    def movement_update(self):\n",
    "        # if self.x == 880:\n",
    "        #     self.dir = 'l'\n",
    "        # elif self.x == 0:\n",
    "        #     self.dir = 'r'\n",
    "\n",
    "        if self.dir == 'r':                        \n",
    "            self.x += FPS\n",
    "        else:\n",
    "            self.image = pygame.transform.flip(self.image, True, False)\n",
    "            self.x -= FPS\n",
    "        screen.blit(self.image, (self.x, self.y))\n",
    " \n",
    "    def update(self):\n",
    "        # print(self.index, self.action_count, len(self.actions), len(self.images[self.actions[self.action_count]]))\n",
    "        print(self.actions[self.action_count])\n",
    "        if self.actions[self.action_count] == \"say\":\n",
    "            global dialogue_count\n",
    "            self.play_dialogue(dialogue_count)\n",
    "            dialogue_count += 1\n",
    "            print(\"dialogie played\")\n",
    "        if self.index >= len(self.images[self.actions[self.action_count]]):\n",
    "            if self.action_count >= len(self.actions) - 1:\n",
    "                return 1\n",
    "            self.index = 0\n",
    "            self.action_count += 1\n",
    "        self.image = self.images[self.actions[self.action_count]][self.index]\n",
    "        self.index += 1\n",
    "\n",
    "        if actions_movement[self.actions[self.action_count]]:\n",
    "            self.movement_update()\n",
    "        else:\n",
    "            screen.blit(self.image, (self.x, self.y))\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def update_idle(self):\n",
    "        self.image = pygame.transform.scale(pygame.image.load(\"pygame\\\\\" + self.char + \"\\\\\" + \"idle1.png\") , (200,200))\n",
    "        flip_var = True if self.dir == \"l\" else False\n",
    "        print(flip_var)\n",
    "        self.image = pygame.transform.flip(self.image, flip_var, False)\n",
    "        screen.blit(self.image, (self.x, self.y))\n",
    "        \n",
    " \n",
    "def main():\n",
    "    pygame.init()\n",
    "    pygame.display.set_caption(\"Trace\")\n",
    "    char_objects = []\n",
    "    for i in range(len(charcaters)):\n",
    "        dir = list(dir_list.keys())[i % 2]\n",
    "        char_objects.append(MySprite(charcaters[i], dir_list[dir] * ((i % 2) + 1), 350, dir))\n",
    "    # my_group = pygame.sprite.Group(my_sprite)\n",
    "    idle_char = []\n",
    "    clock = pygame.time.Clock()\n",
    "    \n",
    "    loop = 1\n",
    "    count = 0 # number of characters whos actions have been completed\n",
    "\n",
    "\n",
    "    return_val = 0\n",
    "    while count < len(charcaters): \n",
    "\n",
    "        pygame.event.get() \n",
    "        \n",
    "        screen.fill((0,0,0))        \n",
    "        screen.blit(bg, (0, 0))\n",
    "\n",
    "        for char in char_objects:\n",
    "            return_val = char.update()   \n",
    "            print(return_val) \n",
    "            if return_val == 1:\n",
    "                char_objects.remove(char)\n",
    "                idle_char.append(char)\n",
    "                count += return_val\n",
    "        \n",
    "        for char in idle_char:\n",
    "            print(char.x)\n",
    "            char.update_idle()\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(FPS)\n",
    "\n",
    "    pygame.quit()\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import spacy.cli\n",
    "# Download following once\n",
    "#spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CS', 24, 26, 'ORG'), ('MIT', 48, 51, 'ORG')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Victoria was a major in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Victoria studies at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MIT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Victoria was a major in CS. Victoria studies at MIT.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "# now we use displaycy function on doc2\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "  \n",
    "# This module is imported so that we can \n",
    "# play the converted audio\n",
    "import os\n",
    "  \n",
    "# The text that you want to convert to audio\n",
    "mytext = 'Welcome to geeksforgeeks!'\n",
    "  \n",
    "# Language in which you want to convert\n",
    "language = 'en'\n",
    "  \n",
    "# Passing the text and language to the engine, \n",
    "# here we have marked slow=False. Which tells \n",
    "# the module that the converted audio should \n",
    "# have a high speed\n",
    "myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "  \n",
    "# Saving the converted audio in a mp3 file named\n",
    "# welcome \n",
    "myobj.save(\"welcome.mp3\")\n",
    "  \n",
    "# Playing the converted file\n",
    "os.system(\"mpg321 welcome.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is walking on a sunny day. A cat jumped. A cat was talking with a human and died. Dog is walking in the opposite direction. Dog ran.\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token is POS: AUX, dep: aux\n",
      "Token walking POS: VERB, dep: ROOT\n",
      "Token on POS: ADP, dep: prep\n",
      "Token a POS: DET, dep: det\n",
      "Token sunny POS: ADJ, dep: amod\n",
      "Token day POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token jumped POS: VERB, dep: ROOT\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token was POS: AUX, dep: aux\n",
      "Token talking POS: VERB, dep: ROOT\n",
      "Token with POS: ADP, dep: prep\n",
      "Token a POS: DET, dep: det\n",
      "Token human POS: NOUN, dep: pobj\n",
      "Token and POS: CCONJ, dep: cc\n",
      "Token died POS: VERB, dep: conj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token Dog POS: PROPN, dep: nsubj\n",
      "Token is POS: AUX, dep: aux\n",
      "Token walking POS: VERB, dep: ROOT\n",
      "Token in POS: ADP, dep: prep\n",
      "Token the POS: DET, dep: det\n",
      "Token opposite POS: ADJ, dep: amod\n",
      "Token direction POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token Dog POS: PROPN, dep: nsubj\n",
      "Token ran POS: VERB, dep: ROOT\n",
      "Token . POS: PUNCT, dep: punct\n",
      "svo:, subject: a cat a cat a cat dog dog, verb: walking jumped talking died walking ran, attribute: , question: False, wh_word: \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# object and subject constants\n",
    "OBJECT_DEPS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"agent\", \"expl\"}\n",
    "# tags that define wether the word is wh-\n",
    "WH_WORDS = {\"WP\", \"WP$\", \"WRB\"}\n",
    "\n",
    "# extract the subject, object and verb from the input\n",
    "def extract_svo(doc):\n",
    "    sub = []\n",
    "    at = []\n",
    "    ve = []\n",
    "    for token in doc:\n",
    "        # is this a verb?\n",
    "        if token.pos_ == \"VERB\":\n",
    "            ve.append(token.text)\n",
    "        # is this the object?\n",
    "        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:\n",
    "            at.append(token.text)\n",
    "        # is this the subject?\n",
    "        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS:\n",
    "            sub.append(token.text)\n",
    "    return \" \".join(sub).strip().lower(), \" \".join(ve).strip().lower(), \" \".join(at).strip().lower()\n",
    "\n",
    "# wether the doc is a question, as well as the wh-word if any\n",
    "def is_question(doc):\n",
    "    # is the first token a verb?\n",
    "    if len(doc) > 0 and doc[0].pos_ == \"VERB\":\n",
    "        return True, \"\"\n",
    "    # go over all words\n",
    "    for token in doc:\n",
    "        # is it a wh- word?\n",
    "        if token.tag_ in WH_WORDS:\n",
    "            return True, token.text.lower()\n",
    "    return False, \"\"\n",
    "\n",
    "# gather the user input and gather the info\n",
    "doc = nlp(\"A cat is walking on a sunny day. A cat jumped. A cat was talking with a human and died. Dog is walking in the opposite direction. Dog ran.\")\n",
    "print(doc)\n",
    "# print out the pos and deps\n",
    "for token in doc:\n",
    "    print(\"Token {} POS: {}, dep: {}\".format(token.text, token.pos_, token.dep_))\n",
    "\n",
    "# get the input information\n",
    "subject, verb, attribute = extract_svo(doc)\n",
    "question, wh_word = is_question(doc)\n",
    "print(\"svo:, subject: {}, verb: {}, attribute: {}, question: {}, wh_word: {}\".format(subject, verb, attribute, question, wh_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3440fa40ac2c39878b83ee7a6e81824601a62799733fc81e385c4ef9c8700d77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
