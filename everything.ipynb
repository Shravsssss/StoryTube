{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coref resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install allennlp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "cannot instantiate 'WindowsPath' on your system",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# load saved model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path , \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 7\u001b[0m     predictor \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m      9\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mA cat is walking on a sunny day. It jumped over a stone. It died. Dog is walking in the opposite direction. It ran. The dog said \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHello world\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m prediction \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mpredict(document\u001b[39m=\u001b[39mtext)  \u001b[39m# get prediction\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.10/pathlib.py:962\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_parts(args)\n\u001b[1;32m    961\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flavour\u001b[39m.\u001b[39mis_supported:\n\u001b[0;32m--> 962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot instantiate \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m on your system\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m                               \u001b[39m%\u001b[39m (\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,))\n\u001b[1;32m    964\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: cannot instantiate 'WindowsPath' on your system"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path, PosixPath\n",
    "\n",
    "path = PosixPath('./allen_coref')\n",
    "# load saved model\n",
    "with open(path , 'rb') as f:\n",
    "    predictor = pickle.load(f)\n",
    "\n",
    "text = \"\"\"A cat is walking on a sunny day. It jumped over a stone. It died. Dog is walking in the opposite direction. It ran. The dog said \"Hello world\".\"\"\"\n",
    "\n",
    "prediction = predictor.predict(document=text)  # get prediction\n",
    "print(\"Clsuters:-\")\n",
    "for cluster in prediction['clusters']:\n",
    "    print(cluster)  # list of clusters (the indices of spaCy tokens)\n",
    "# Result: [[[0, 3], [26, 26]], [[34, 34], [50, 50]]]\n",
    "print('\\n\\n') #Newline\n",
    "\n",
    "coref_text = predictor.coref_resolved(text)\n",
    "\n",
    "print('Coref resolved: ', coref_text)  # resolved text\n",
    "# Result: Joseph Robinette Biden Jr. is an American politician who is the 46th and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAO parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textacy\n",
    "pip install stanford_openie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is walking on a sunny day. A cat jumped over a stone. A cat died. Dog is walking in the opposite direction. Dog ran. Dog said \"Hello world\".\n",
      "[('A cat', 'walking'), ('A cat', 'jumped'), ('A cat', 'died'), ('Dog', 'walking'), ('Dog', 'ran'), ('Dog', 'said', 'Hello world')]\n"
     ]
    }
   ],
   "source": [
    "# from subject_verb_object_extract import findSVOs, nlp\n",
    "# coref_text = \"A cat is walk rapidly on a sunny day. A cat fall on a stone. A cat dead\"\n",
    "# tokens = nlp(coref_text)\n",
    "# SVOs = findSVOs(tokens)\n",
    "# print(coref_text)\n",
    "# print(SVOs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': ['walking', 'jumped', 'died'], 'dog': ['walking', 'ran', ['said', 'Hello world']]}\n",
      "['cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from collections import defaultdict\n",
    "# from nltk.tokenize import word_tokenize\n",
    "  \n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# svos = defaultdict(list)\n",
    "\n",
    "# for svo in SVOs: \n",
    "#     subject = svo[0]\n",
    "#     word_tokens = word_tokenize(subject)\n",
    "#     filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
    "#     if svo[1] == \"said\":\n",
    "#         verb = []\n",
    "#         verb.append(svo[1])\n",
    "#         verb.append(svo[2])\n",
    "#     else:\n",
    "#         verb = svo[1]\n",
    "#     svos[\"\".join(filtered_sentence)].append(verb)\n",
    "    \n",
    "# svos = dict(svos)\n",
    "# print(svos)\n",
    "\n",
    "# charcaters = list(i.lower() for i in svos.keys())\n",
    "# print(charcaters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClausIE installation\n",
    "python -m pip install git+https://github.com/mmxgn/spacy-clausie.git\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "## ClausIE usage\n",
    "A clause is a part of a sentence that expresses some coherent piece of information; it consists of one subject (S), one\n",
    "verb (V), and optionally of an indirect object (O), a direct\n",
    "object (O), a complement (C), and one or more adverbials\n",
    "(A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<SV, A cat, is walk, None, None, None, [rapidly, on a sunny day]>, <SVA, A cat, fall, None, None, None, [on a stone]>]\n",
      "[['A cat is walk rapidly on a sunny day', 'A cat is walk on a sunny day', 'A cat is walk rapidly'], ['A cat fell on a stone']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import claucy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "claucy.add_to_pipe(nlp)\n",
    "\n",
    "coref_text = \"A cat is walk rapidly on a sunny day. A cat fall on a stone. A cat dead\"\n",
    "doc = nlp(coref_text)\n",
    "\n",
    "SVOs = doc._.clauses\n",
    "print(SVOs)\n",
    "\n",
    "propositions = [SVO.to_propositions(as_text=True) for SVO in SVOs]\n",
    "\n",
    "print(propositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "actions_movement = {'die' : 0, 'fall' : 0, 'hurt' : 0, 'idle' : 0, 'jump' : 1, 'run' : 1, 'slide' : 1, 'walk' : 1, 'say': 0}\n",
    "#word2vec similarity between the incoming action vs the ones we have in list of actions and then set a threshold and execute the action based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"said\",'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walk']\n",
      "['walk', 'jump']\n",
      "['walk', 'jump', 'die']\n",
      "{'walk': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'jump': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'die': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>]}\n",
      "['walk']\n",
      "['walk', 'run']\n",
      "['walk', 'run', 'say']\n",
      "{'walk': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'run': [<Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>, <Surface(200x200x32 SW)>], 'say': [<Surface(200x200x32 SW)>]}\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "walk\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "jump\n",
      "0\n",
      "run\n",
      "0\n",
      "die\n",
      "0\n",
      "say\n",
      "dialogie played\n",
      "1\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "0\n",
      "674\n",
      "True\n",
      "die\n",
      "1\n",
      "674\n",
      "True\n",
      "326\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from numpy import character\n",
    "import pygame\n",
    "import glob\n",
    "from time import sleep\n",
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "SIZE = WIDTH, HEIGHT = 1280, 720 #the width and height of our screen\n",
    "FPS = 7 #Frames per second\n",
    "\n",
    "screen = pygame.display.set_mode(SIZE)\n",
    "bg = pygame.transform.scale(pygame.image.load('pygame\\sunny_day.png') , SIZE)\n",
    "\n",
    "dir_list = {'r' : 200, 'l' : 400} #moving towards which direction\n",
    "\n",
    "dialogues = []\n",
    "dialogue_count = 0\n",
    " \n",
    "class MySprite(pygame.sprite.Sprite):\n",
    "\n",
    "    def __init__(self, char, x, y, dir):\n",
    "        super(MySprite, self).__init__()\n",
    "        \n",
    "        self.images = {}\n",
    "        self.actions = []\n",
    "        # self.images = [pygame.image.load(img) for img in glob.glob(\"pygame\\\\cat\\\\*.png\")]\n",
    "        for action in svos[char]:\n",
    "            if type(action) == list:\n",
    "                dialogues.append(action[1])\n",
    "            action = WordNetLemmatizer().lemmatize(action[0] if type(action) == list else action,'v')   \n",
    "            self.actions.append(action)\n",
    "            print(self.actions)\n",
    "            \n",
    "            if action not in self.images:\n",
    "                self.images[action] = [pygame.transform.scale(pygame.image.load(img) , (200,200)) for img in glob.glob(\"pygame\\\\\" + char + \"\\\\\" + action + \"*.png\")]\n",
    "            \n",
    "        print(self.images)\n",
    "        self.char = char\n",
    "        self.index = 0\n",
    "        self.action_count = 0\n",
    "        self.dir = dir\n",
    "        self.x = x\n",
    "        self.y = y   \n",
    "\n",
    "    def play_dialogue(self, index):\n",
    "        myobj = gTTS(text=dialogues[index], lang=language, slow=False)\n",
    "        myobj.save(\"dialogue.mp3\")\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(\"dialogue.mp3\")\n",
    "        pygame.mixer.music.set_volume(1)\n",
    "        pygame.mixer.music.play()\n",
    "        \n",
    "    def movement_update(self):\n",
    "        # if self.x == 880:\n",
    "        #     self.dir = 'l'\n",
    "        # elif self.x == 0:\n",
    "        #     self.dir = 'r'\n",
    "\n",
    "        if self.dir == 'r':                        \n",
    "            self.x += FPS\n",
    "        else:\n",
    "            self.image = pygame.transform.flip(self.image, True, False)\n",
    "            self.x -= FPS\n",
    "        screen.blit(self.image, (self.x, self.y))\n",
    " \n",
    "    def update(self):\n",
    "        # print(self.index, self.action_count, len(self.actions), len(self.images[self.actions[self.action_count]]))\n",
    "        print(self.actions[self.action_count])\n",
    "        if self.actions[self.action_count] == \"say\":\n",
    "            global dialogue_count\n",
    "            self.play_dialogue(dialogue_count)\n",
    "            dialogue_count += 1\n",
    "            print(\"dialogie played\")\n",
    "        if self.index >= len(self.images[self.actions[self.action_count]]):\n",
    "            if self.action_count >= len(self.actions) - 1:\n",
    "                return 1\n",
    "            self.index = 0\n",
    "            self.action_count += 1\n",
    "        self.image = self.images[self.actions[self.action_count]][self.index]\n",
    "        self.index += 1\n",
    "\n",
    "        if actions_movement[self.actions[self.action_count]]:\n",
    "            self.movement_update()\n",
    "        else:\n",
    "            screen.blit(self.image, (self.x, self.y))\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def update_idle(self):\n",
    "        self.image = pygame.transform.scale(pygame.image.load(\"pygame\\\\\" + self.char + \"\\\\\" + \"idle1.png\") , (200,200))\n",
    "        flip_var = True if self.dir == \"l\" else False\n",
    "        print(flip_var)\n",
    "        self.image = pygame.transform.flip(self.image, flip_var, False)\n",
    "        screen.blit(self.image, (self.x, self.y))\n",
    "        \n",
    " \n",
    "def main():\n",
    "    pygame.init()\n",
    "    pygame.display.set_caption(\"Trace\")\n",
    "    char_objects = []\n",
    "    for i in range(len(charcaters)):\n",
    "        dir = list(dir_list.keys())[i % 2]\n",
    "        char_objects.append(MySprite(charcaters[i], dir_list[dir] * ((i % 2) + 1), 350, dir))\n",
    "    # my_group = pygame.sprite.Group(my_sprite)\n",
    "    idle_char = []\n",
    "    clock = pygame.time.Clock()\n",
    "    \n",
    "    loop = 1\n",
    "    count = 0 # number of characters whos actions have been completed\n",
    "\n",
    "\n",
    "    return_val = 0\n",
    "    while count < len(charcaters): \n",
    "\n",
    "        pygame.event.get() \n",
    "        \n",
    "        screen.fill((0,0,0))        \n",
    "        screen.blit(bg, (0, 0))\n",
    "\n",
    "        for char in char_objects:\n",
    "            return_val = char.update()   \n",
    "            print(return_val) \n",
    "            if return_val == 1:\n",
    "                char_objects.remove(char)\n",
    "                idle_char.append(char)\n",
    "                count += return_val\n",
    "        \n",
    "        for char in idle_char:\n",
    "            print(char.x)\n",
    "            char.update_idle()\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(FPS)\n",
    "\n",
    "    pygame.quit()\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import spacy.cli\n",
    "# Download following once\n",
    "#spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CS', 24, 26, 'ORG'), ('MIT', 48, 51, 'ORG')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Victoria was a major in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Victoria studies at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MIT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Victoria was a major in CS. Victoria studies at MIT.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "# now we use displaycy function on doc2\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "  \n",
    "# This module is imported so that we can \n",
    "# play the converted audio\n",
    "import os\n",
    "  \n",
    "# The text that you want to convert to audio\n",
    "mytext = 'Welcome to geeksforgeeks!'\n",
    "  \n",
    "# Language in which you want to convert\n",
    "language = 'en'\n",
    "  \n",
    "# Passing the text and language to the engine, \n",
    "# here we have marked slow=False. Which tells \n",
    "# the module that the converted audio should \n",
    "# have a high speed\n",
    "myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "  \n",
    "# Saving the converted audio in a mp3 file named\n",
    "# welcome \n",
    "myobj.save(\"welcome.mp3\")\n",
    "  \n",
    "# Playing the converted file\n",
    "os.system(\"mpg321 welcome.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is walking on a sunny day. A cat jumped. A cat was talking with a human and died. Dog is walking in the opposite direction. Dog ran.\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token is POS: AUX, dep: aux\n",
      "Token walking POS: VERB, dep: ROOT\n",
      "Token on POS: ADP, dep: prep\n",
      "Token a POS: DET, dep: det\n",
      "Token sunny POS: ADJ, dep: amod\n",
      "Token day POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token jumped POS: VERB, dep: ROOT\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token A POS: DET, dep: det\n",
      "Token cat POS: NOUN, dep: nsubj\n",
      "Token was POS: AUX, dep: aux\n",
      "Token talking POS: VERB, dep: ROOT\n",
      "Token with POS: ADP, dep: prep\n",
      "Token a POS: DET, dep: det\n",
      "Token human POS: NOUN, dep: pobj\n",
      "Token and POS: CCONJ, dep: cc\n",
      "Token died POS: VERB, dep: conj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token Dog POS: PROPN, dep: nsubj\n",
      "Token is POS: AUX, dep: aux\n",
      "Token walking POS: VERB, dep: ROOT\n",
      "Token in POS: ADP, dep: prep\n",
      "Token the POS: DET, dep: det\n",
      "Token opposite POS: ADJ, dep: amod\n",
      "Token direction POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token Dog POS: PROPN, dep: nsubj\n",
      "Token ran POS: VERB, dep: ROOT\n",
      "Token . POS: PUNCT, dep: punct\n",
      "svo:, subject: a cat a cat a cat dog dog, verb: walking jumped talking died walking ran, attribute: , question: False, wh_word: \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# object and subject constants\n",
    "OBJECT_DEPS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"agent\", \"expl\"}\n",
    "# tags that define wether the word is wh-\n",
    "WH_WORDS = {\"WP\", \"WP$\", \"WRB\"}\n",
    "\n",
    "# extract the subject, object and verb from the input\n",
    "def extract_svo(doc):\n",
    "    sub = []\n",
    "    at = []\n",
    "    ve = []\n",
    "    for token in doc:\n",
    "        # is this a verb?\n",
    "        if token.pos_ == \"VERB\":\n",
    "            ve.append(token.text)\n",
    "        # is this the object?\n",
    "        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:\n",
    "            at.append(token.text)\n",
    "        # is this the subject?\n",
    "        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS:\n",
    "            sub.append(token.text)\n",
    "    return \" \".join(sub).strip().lower(), \" \".join(ve).strip().lower(), \" \".join(at).strip().lower()\n",
    "\n",
    "# wether the doc is a question, as well as the wh-word if any\n",
    "def is_question(doc):\n",
    "    # is the first token a verb?\n",
    "    if len(doc) > 0 and doc[0].pos_ == \"VERB\":\n",
    "        return True, \"\"\n",
    "    # go over all words\n",
    "    for token in doc:\n",
    "        # is it a wh- word?\n",
    "        if token.tag_ in WH_WORDS:\n",
    "            return True, token.text.lower()\n",
    "    return False, \"\"\n",
    "\n",
    "# gather the user input and gather the info\n",
    "doc = nlp(\"A cat is walking on a sunny day. A cat jumped. A cat was talking with a human and died. Dog is walking in the opposite direction. Dog ran.\")\n",
    "print(doc)\n",
    "# print out the pos and deps\n",
    "for token in doc:\n",
    "    print(\"Token {} POS: {}, dep: {}\".format(token.text, token.pos_, token.dep_))\n",
    "\n",
    "# get the input information\n",
    "subject, verb, attribute = extract_svo(doc)\n",
    "question, wh_word = is_question(doc)\n",
    "print(\"svo:, subject: {}, verb: {}, attribute: {}, question: {}, wh_word: {}\".format(subject, verb, attribute, question, wh_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
